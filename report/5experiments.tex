%!TEX root = report.tex


\section{Experiments}
We implement our model in Section \ref{Sec:methodology} with TensorFlow, 
and apply our code on the dataset described in Section \ref{Sec:ProblemData}. 
To be specific, we train and tune our model on the 2013 Train and Dev dataset, 
and evaluate our model on the Test 2013--2015 dataset. The focus of this section 
is to compare our model performance with the CNN baseline model and 

\subsection{Experiment Setting}
We employ the pretrained word embeddings by Austudillo et al.\ \cite{astudillo2015learning}. 
These embeddings are obtained via training on a corpus of 52 million tweets
using the structure skip-gram model \cite{ling2015two}, and have been shown to
perform very well on sentiment analysis tasks \cite{yang2017attention}.
We also use the same evaluation metric as the SemEval challenge.

We adopt several existing network node embedding methods including DeepWalk \cite{perozzi2014deepwalk},
LINE \cite{tang2015line}, and node2vec \cite{grover2016node2vec}. 


Regarding baseline models, we adopt the state-of-the-art convolutional neural network model
methodology for sentiment analysis. This model is the basis model of our Interaction Model and
has been described in Section \ref{Sec:methodology}. As a second baseline model, we adopt
the Social Attention model \cite{yang2017attention} which models the soft-assignment of
users to communities. 


\paragraph{Parameter tuning} We tune all the hyper parameters on the SemEval 2013 Dev dataset, including
\begin{itemize}
\item number of bigram filters for the CNN models, from \{16, 32, 50, 100\};
\item dropout rate, from \{0.1, 0.2, 0.4\};
\item $L_2$-penalty coefficient, from \{1e-6, 1e-4, 1e-2\}.
\end{itemize}
Additionally for our proposed Interaction Model that combines social network information, we have the following hyper parameters,
\begin{itemize}
\item 
\end{itemize}




\subsection{Results}

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c| c | c c c c|}
\hline
 & CNN & SA
\footnotemark
& DeepWalk & LINE & node2vec & random\\
\hline
Dev2013  & 68.85 &69.52 & 67.71 & 69.51 & 68.58 & 68.50 \\
\hline
Test2013  & 69.53 & 69.98& 67.58 & 69.67 & 68.58 & 68.49 \\
Test2014  & 72.41 & 72.70 & 71.46 & 71.44 & 71.46 & 71.69 \\
Test2015  & 64.40 &65.28 & 64.71 & 64.57 & 64.25 & 63.50 \\
\hline
Avg test sets & 68.78 &69.32& 67.92 & 68.56  & 68.10 & 67.89 \\
\hline
\end{tabular}
\end{center}
\caption{Prediction performance on each Dev and Test Sets. }
\label{Tb:}
\end{table}%
\footnotetext{Social Attention model \cite{Yang}. Note that even after contacting the authors and obtaining their code, 
we still fail to retrieve their declared result in the paper. Here we put the implemented result using their provided code.} 


\subsection{A two-stage training method}